{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef07bca",
   "metadata": {},
   "source": [
    "Pixel value closest to 0 will be more darker(in positive region).Means negative values will represent darker values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a96c7",
   "metadata": {},
   "source": [
    "Dimensions of given matrix - coressponding filter matrix dimensions + 1(gives you the dimension of the final matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659e293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc72d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('train(cd)')\n",
    "test_dir = os.path.join('test(cd)')\n",
    "\n",
    "train_cats = os.path.join(train_dir, 'cats')\n",
    "train_dogs = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "test_cats = os.path.join(test_dir, 'cats')\n",
    "test_dogs = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53515d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat_10.jpg', 'cat_100.jpg', 'cat_101.jpg', 'cat_102.jpg', 'cat_103.jpg', 'cat_104.jpg', 'cat_11.jpg', 'cat_111.jpg', 'cat_12.jpg', 'cat_120.jpg']\n",
      "['dog_0.jpg', 'dog_10.jpg', 'dog_100.jpg', 'dog_102.jpg', 'dog_103.jpg', 'dog_106.jpg', 'dog_109.jpg', 'dog_11.jpg', 'dog_110.jpg', 'dog_113.jpg']\n"
     ]
    }
   ],
   "source": [
    "train_cat_fnames = os.listdir(train_cats)\n",
    "train_dog_fnames = os.listdir(train_dogs)\n",
    "\n",
    "print(train_cat_fnames[:10])\n",
    "print(train_dog_fnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2960c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images:  279\n",
      "total training dogs images:  278\n",
      "total testing cat images:  70\n",
      "total testing dog images:  70\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images: ', len(os.listdir(train_cats)))\n",
    "print('total training dogs images: ', len(os.listdir(train_dogs)))\n",
    "\n",
    "print('total testing cat images: ', len(os.listdir(test_cats)))\n",
    "print('total testing dog images: ', len(os.listdir(test_dogs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ebdeec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1370cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16,(3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32,(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07a077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer= RMSprop(learning_rate=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee5c48",
   "metadata": {},
   "source": [
    "# Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6067ae4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 images belonging to 2 classes.\n",
      "Found 140 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator( \n",
    "    rescale = 1.0/255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    "    )\n",
    "\n",
    "test_datagen = ImageDataGenerator( rescale = 1.0/255.)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                   batch_size=20,\n",
    "                                                   class_mode= 'binary',\n",
    "                                                   target_size=(150,150))\n",
    "\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                  batch_size=20,\n",
    "                                                 class_mode='binary',\n",
    "                                                 target_size=(150,150))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fb303c",
   "metadata": {},
   "source": [
    "verbose is presentation of epochs like if 2 it will show only essential paramters and if 0 it will show none whereas if 1 then it will show wholw animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18933b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amina\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "C:\\Users\\amina\\anaconda3\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 - 30s - 295ms/step - accuracy: 0.4865 - loss: 0.8530 - val_accuracy: 0.5000 - val_loss: 0.6920\n",
      "Epoch 2/15\n",
      "100/100 - 16s - 164ms/step - accuracy: 0.5296 - loss: 0.6924 - val_accuracy: 0.5071 - val_loss: 0.6920\n",
      "Epoch 3/15\n",
      "100/100 - 18s - 182ms/step - accuracy: 0.5835 - loss: 0.7070 - val_accuracy: 0.5000 - val_loss: 0.7280\n",
      "Epoch 4/15\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                   validation_data=test_generator,\n",
    "                   steps_per_epoch=100,\n",
    "                   epochs=15,\n",
    "                   validation_steps=50,\n",
    "                   verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.input_shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc)\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Training and testing accuracy')\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.plot(epochs, val_loss)\n",
    "plt.title('Training and testing loss')\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef36001",
   "metadata": {},
   "source": [
    "# Separate model testing for any image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_prepare_image(img_path, target_size=(150, 150)):\n",
    "    \"\"\"\n",
    "    Load and preprocess the image to the required input size.\n",
    "    \"\"\"\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) \n",
    "    img_array /= 255.0  \n",
    "    return img_array\n",
    "\n",
    "\n",
    "img_path = 'path.jpeg' \n",
    "\n",
    "\n",
    "img_array = load_and_prepare_image(img_path)\n",
    "\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "\n",
    "if predictions[0] > 0.5:\n",
    "    print(\"Predicted: Dog\")\n",
    "else:\n",
    "    print(\"Predicted: Cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
